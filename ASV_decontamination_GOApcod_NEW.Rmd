---
title: "ASV decontamination of GOA pcod eDNA samples"
author: "Kimberly Ledger"
date: "2023-06-19"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

this script is an updated version of my ASV decontamination pipeline for the GOA pcod 2021 samples. 

load libraries 
```{r}
library(tidyverse)
library(fitdistrplus) #for fitdist()
```

the data i am starting with is the ASV table output from DADA2 pipeline in my sequence_filtering.Rmd in the eDNA_metabarcoding folder
```{r}
#NOAA VM
#asv_table <- read.csv("/genetics/edna/workdir/GOApcod_2021/combined/trimmed/filtered/outputs/ASVtable.csv") %>%
#  rename(Sample_ID = X)

#my mac
asv_table <- read.csv("data/ASVtable.csv") %>%
  rename(Sample_ID = X)

asv_table$Sample_ID <- as.factor(asv_table$Sample_ID)
```

starting number of ASV's = 1859

# 1. Estimate index hopping  
subtract the proportion of reads that jumped into the control samples from each environmental sample 

we need sample metadata to do this... 
```{r}
#NOAA VM
#metadata <- read.csv("/genetics/edna/workdir/GOApcod_2021/GOA2021_metadata_20230515.csv")

#my mac
metadata <- read.csv("data/GOA2021_metadata_20230515.csv")

#illumina output changed "_" to "-"
metadata$Sample_ID <- gsub("_", "-", metadata$Sample_ID) 
```

add column to asv table that labels the sample type
```{r}
samp_type <- metadata %>%
  dplyr::select(Sample_ID, sample_type) %>%
  left_join(asv_table, by = "Sample_ID")
```

identify maximum proportion of reads for each ASV found in the positive controls
```{r}
pos_asvs <- samp_type %>%
  filter(sample_type == "positive_control") %>%
  pivot_longer(cols = c(3:1861), names_to = "ASV", values_to = "reads") %>%
  group_by(Sample_ID) %>%
  mutate(TotalReadsPerSample = sum(reads)) %>%
  mutate(Prop = reads/TotalReadsPerSample) %>%
  group_by(ASV) %>%
  summarise(max_prop = max(Prop))
```

now subtract this max proportion for each ASV from environmental samples and all negative controls  
```{r}
indexhop_table <- samp_type %>%
  filter(sample_type != "positive_control") %>% ## working all samples except the positive controls 
  pivot_longer(cols = c(3:1861), names_to = "ASV", values_to = "reads") %>%
  group_by(Sample_ID) %>%
  mutate(TotalReadsPerSample = sum(reads)) %>%
  left_join(pos_asvs, by = "ASV") %>%
  mutate(IndexHoppingReads = TotalReadsPerSample*max_prop) %>%
  mutate(reads_IndexHop_removed = reads - IndexHoppingReads) %>%
  mutate(reads_IndexHop_removed = if_else(reads_IndexHop_removed < 0, 0, reads_IndexHop_removed))
```

clean up the table by removing columns no longer needed 
```{r}
asv_table_filter1 <- indexhop_table %>%
  dplyr::select(Sample_ID, sample_type, ASV, reads_IndexHop_removed) %>%
  rename(reads = reads_IndexHop_removed)
head(asv_table_filter1)
```

this is a summary of the number of reads removed by ASV and sample_ID
```{r}
reads_removed1 <- indexhop_table %>%
  dplyr::select(Sample_ID, ASV, IndexHoppingReads) %>%
  pivot_wider(names_from = "ASV", values_from = "IndexHoppingReads")
head(reads_removed1)
```

and a list of the proportion of reads from ASVs removed 
```{r}
prop_removed1 <- pos_asvs %>%
  arrange(desc(max_prop))
head(prop_removed1)
```

apart from ASV16 which is for sturgeon (the positive control) and a few ASVs at about 2.5%, the max proportion of reads removed is just over 1% 

- this decontamination steps seems to be working okay. 


# 2. Discard PCR replicates with low numbers of reads 

calculate reads per sample

**just consider the field blanks and environmental samples here???** 
**or should i include extraction and pcr negatives??**  
when i include the pcr and extraction negatives, it seems like most have low reads and get discarded... 

```{r}
all_reads <- asv_table_filter1 %>%
# filter(sample_type == "field_blank" | sample_type == "sample") %>%
  group_by(Sample_ID) %>%
  summarize(ReadsPerSample = sum(reads))
```

fit a normal distribution
```{r}
fit <- fitdist(all_reads$ReadsPerSample, "gamma", lower=c(0,0), start=list(scale=1,shape=1))

all_reads %>%  
  mutate(prob = pgamma(ReadsPerSample, shape = fit$estimate[[2]], scale = fit$estimate[[1]], lower.tail = TRUE,
       log.p = FALSE)) -> all_reads
```

identify and remove the outliers - parameters used in gruinard_decon
```{r}
low_dist_probability_cutoff <- 0.05
minimum_read_cutoff <- 1000

outliers <- all_reads %>% 
  filter(prob < low_dist_probability_cutoff  | ReadsPerSample < minimum_read_cutoff) # changed to 0.05 to save the two samples
outlierIDs <- outliers$Sample_ID
```


which samples are removed because of the 5%/1000 reads threshold??
```{r}
samples_removed <- asv_table_filter1 %>%
  filter(Sample_ID %in% outlierIDs) %>%
  pivot_wider(names_from = "ASV", values_from = "reads")
head(samples_removed)
```

plot them
```{r}
samples_removed %>%
  pivot_longer(cols = c(3:143), names_to = "ASV", values_to = "count") %>%
ggplot(aes(x=Sample_ID, y=count, fill=ASV)) +
  geom_bar(stat = "identity") + 
    theme_bw() + 
  theme(
    axis.text.x = element_blank(),
    legend.position = "none",
    legend.title = element_blank()
  )  
```

eek. some of these samples definitely should not get removed! maybe go with an absolute threshold or drop the probability threshold??? 

i could probably reduce the 5% threshold (maybe to 2%) or just go with the 1000 read threshold. 

identify and remove the outliers - parameters used in gruinard_decon
```{r}
low_dist_probability_cutoff <- 0.02
minimum_read_cutoff <- 1000

outliers <- all_reads %>% 
  filter(prob < low_dist_probability_cutoff  | ReadsPerSample < minimum_read_cutoff) # changed to 0.05 to save the two samples
outlierIDs <- outliers$Sample_ID
```


which samples are removed because of the 5%/1000 reads threshold??
```{r}
samples_removed <- asv_table_filter1 %>%
  filter(Sample_ID %in% outlierIDs) %>%
  pivot_wider(names_from = "ASV", values_from = "reads")
head(samples_removed)
```

plot them
```{r}
samples_removed %>%
  pivot_longer(cols = c(3:143), names_to = "ASV", values_to = "count") %>%
ggplot(aes(x=Sample_ID, y=count, fill=ASV)) +
  geom_bar(stat = "identity") + 
    theme_bw() + 
  theme(
    axis.text.x = element_blank(),
    legend.position = "none",
    legend.title = element_blank()
  )  
```

okay, will go with this for now.  but maybe just use 1000 read threshold??? 


filter the data frame 
```{r}
asv_table_filter2 <- asv_table_filter1 %>%
  filter(!Sample_ID %in% outlierIDs)
```


# 3. Account for contaminants in negative controls 

following this from Zack Gold: 
“For each ASV, we calculate the maximum proportion, mean proportion, total number of reads, and prevalence of reads in all samples. If all the statistics are higher in either the positive or environmental samples, we label the ASV as a control or environmental sequence. If the statistics conflict, we only remove the ASVs that have maximum abundance in the positive controls.” 

```{r}
tempA <- asv_table_filter2 %>%
  group_by(ASV, sample_type) %>%
  summarize(TotalReadsPerASV = sum(reads)) %>%
  arrange(ASV)
```

```{r}
tempB <- asv_table_filter2 %>%
  group_by(Sample_ID) %>%
  mutate(TotalReadsPerSample = sum(reads)) %>%
  mutate(Prop = reads/TotalReadsPerSample) %>%
  group_by(sample_type, ASV) %>%
  summarise(max_prop = max(Prop),
            mean_prop = mean(Prop))
```

```{r}
tempC <- tempA %>%
  left_join(tempB, by = c("sample_type", "ASV")) %>%
  pivot_wider(names_from = "sample_type", values_from = c("TotalReadsPerASV", "max_prop", "mean_prop"))
```


note: did not calculate prevalence of reads as stated above (since i'm a little confused on what that is refering too...)

what ASVs have greater read numbers in extraction blanks than samples? 
```{r}
tempC %>%
  filter(TotalReadsPerASV_extraction_blank > TotalReadsPerASV_sample)
```

what ASVs have a max prop > in extraction blanks than samples? 
```{r}
tempC %>%
  filter(max_prop_extraction_blank > max_prop_sample)
```

what ASVs have a mean prop > in extraction blanks than samples? 
```{r}
tempC %>%
  filter(mean_prop_extraction_blank > mean_prop_sample)
```

all the same plus ASV73... 

notes: 
ASV43 is Oncorhynchus nerka. this could be lab contamination.
ASV73 is Homo sapien.  
ASV184 is Oncorhynchus
ASV350 is Oncorhynchus
ASV406 is Oncorhynchus
ASV428 is Oncorhynchus

**all these ASVs seem reasonable to remove**  does this means there is some human and salmon contamination during the extraction process??



what ASVs have greater read numbers in PCR blanks than samples? 
```{r}
tempC %>%
  filter(TotalReadsPerASV_PCR_blank > TotalReadsPerASV_sample)
```

any ASVs max prop > in PCR blanks than samples? 
```{r}
tempC %>%
  filter(max_prop_PCR_blank > max_prop_sample)
```

what about the mean prop? 
```{r}
tempC %>%
  filter(mean_prop_PCR_blank > mean_prop_sample)
```

ASV73 is Homo sapien.  
ASV1262 is Homo
ASV1382 is Homo
ASV1511 is Mammalia
ASV1532 is Primates. 

**should definitely get rid of these** 


what ASVs have greater read numbers in field blanks than samples? 
```{r}
tempC %>%
  filter(TotalReadsPerASV_field_blank > TotalReadsPerASV_sample)
```

any ASVs max prop > in FIELD blanks than samples? 
```{r}
tempC %>%
  filter(max_prop_field_blank > max_prop_sample)
```

what about the mean prop? 
```{r}
tempC %>%
  filter(mean_prop_field_blank > mean_prop_sample)
```

hmmm.... well i can't just toss all these ASVs since it includes my #1 ASV...

maybe. remove any ASV that has a maximum proportion, mean proportion, or total number of reads greater in the EXTRACTION or PCR BLANKS than in the samples. and remove any ASV that has a *total number of reads* greater in the FIELD BLANKS than in real samples.

```{r}
tempD <- tempC %>%
  filter(mean_prop_extraction_blank > mean_prop_sample | mean_prop_PCR_blank > mean_prop_sample | TotalReadsPerASV_field_blank > TotalReadsPerASV_sample)

asvs_to_remove <- tempD$ASV
```

filter the data frame 
```{r}
asv_table_filter3 <- asv_table_filter2 %>%
  filter(!ASV %in% asvs_to_remove)
```

######################################


# 4. Site Occupancy Modeling 


```{r}
sites <- metadata %>%
  dplyr::select(Sample_ID, location1)

rep_table <- asv_table_filter3 %>%
  left_join(sites, by = "Sample_ID") %>%
  arrange(location1)
```

```{r}
library(stringi)
tmp.df <- rep_table %>%
  filter(location1 == 1) # comment this to test the ASV-centric df
```

```{r}
 # format the dataframe appropriately
wide.loc.frame <- tmp.df %>%
  ungroup() %>%  
  mutate(reads = ifelse(reads > 0, 1, 0)) %>% # change counts to presence/absence
    dplyr::select(ASV, Sample_ID, reads) %>%
    group_by(ASV, Sample_ID, reads) %>%
    unique() %>% # eliminate duplicate entries for the same ASV and sample (these seem to be a thing for aquaF2)
    pivot_wider(names_from = Sample_ID, values_from = reads) %>%
  ungroup()
```


```{r}
library(rstan)

 # format must be dataframe, not tibble for converting rownames properly
  wide.loc.frame <- data.frame(wide.loc.frame)

# helper function for maintaining rownames in matrix format
  matrix.please <- function(x) {
    m<-as.matrix(x[,-1])
    rownames(m)<-x[,1]
    m
  }
  
  # convert df to matrix
  wide.loc.matrix <- matrix.please(wide.loc.frame)

n_replicates = ncol(wide.loc.matrix)

# then call the Stan code to work on the dataset we created above. 
# reformatting our data from above to match the Stan code's inputs
testData <<- 
    data.frame(K = n_replicates,   #trials per species (row)
               N = rowSums(wide.loc.matrix),  #detections per species
               z = ifelse(rowSums(wide.loc.matrix) > 0, 1, 0)  #was it ever detected at this site? (integer that helps estimate psi)
    )
  
  mySOMmodel <- stan(file = "Stan_SOM_demo.stan",
                     data = list(
                       S = nrow(testData),
                       K = testData$K,
                       N = testData$N,
                       z = ifelse(testData$N > 0, 1, 0)
                     ),
                     chains = 4,   #number of chains
                     iter = 5000   #number of iterations per chain
  )
  
```

this plot doesn't run because of problem with package installation
```{r}
library(tidybayes)
library(broom)

tmp.plot <- broom::tidy(mySOMmodel) %>%
  filter(!term %in% c("psi", "p11", "p10")) %>%
  arrange(desc(estimate)) %>%
  ggplot(aes(x = reorder(term, estimate), y = estimate)) +
  geom_point() +
  theme(
    axis.text.x = element_blank()
  )

print(tmp.plot)
```


```{r}
library(bayesplot)

  p1 <- mcmc_areas(mySOMmodel, 
                   pars = c("psi", "p11", "p10"))
  p2 <- mcmc_intervals(mySOMmodel, 
                       regex_pars = "Occupancy_prob")
  
plot(p1)
plot(p2)
```

```{r}
#Fit Summary
#fit_summary <- extract(mySOMmodel)


#, probs = c(0.025, 0.975))$summary 
```



```{r}
# also need to save the output in a form that connects the ASVs to the occupancy estimates
  
# grab the name of the ASVs
asvs <- data.frame(wide.loc.frame[,1]) %>%
  dplyr::rename(ASV = wide.loc.frame...1.)

asv.sodm.output <- as.array(mySOMmodel, pars = c("psi", "p11", "p10")) %>%
  as.data.frame() %>%
  dplyr::bind_cols(., asvs) 

  
# combine the ASV names with the sodm output
asv.sodm.output <- tidy(mySOMmodel) %>%
  filter(!term %in% c("psi", "p11", "p10")) %>%
  dplyr::bind_cols(., asvs) 
  
  # write that to a CSV for now
 # asv.sodm.output %>%
 #   write_csv(paste0("csv_outputs/SODM/",loc,"/",site,"_",loc,"_ASV_SODM_5Kiter.csv"))
```







```{r}
# must have run modelText prior to this working
write.table(modelText, "Stan_SOM_allPossibilities.stan", row.names = F, quote = F, col.names = F)
SOMmodel_0_10_detections <- stan(file = "Stan_SOM_allPossibilities.stan", 
                                   data = list(
                                     S = nrow(testData),
                                     K = testData$K,
                                     N = testData$N,
                                     z = ifelse(testData$N > 0, 1, 0)
                                   ), 
                                   chains = 4,   #number of chains
                                   iter = 100000   #number of iterations per chain
  )
  
  # print the output of that plot
  # plot.occu <- mcmc_intervals(SOMmodel_0_10_detections, 
  #                             regex_pars = "Occupancy_prob") +
  #   xlab("Probability of Occupancy") +
  #   ylab("Number of detections (from 0 to 10 out of 10)")
  # 
  # ggsave(paste0("pdf_outputs/SODM/occupancy_tax_only_",loc,"_",site,"_5kiter.pdf"))
```





my make-shift attempt at this...  

**need to come back and figure out how to do this for real...**

right now this code retains only ASVs present in > or = 0.3 of the site replicates (most sites have 3 bio reps with 3 pcr reps)

```{r}
sites <- metadata %>%
  dplyr::select(Sample_ID, location1)

rep_table <- asv_table_filter3 %>%
  #filter(ASV == "ASV1") %>%
  left_join(sites, by = "Sample_ID") %>%
  arrange(location1)

# okay, by doing the left_join i reintroduced the NA sites.... 
rep_table <- rep_table %>%
  filter(location1 != "NA")

### i used this next section of code with just ASV1 to determine the replicate pattern 
rep_counts <- data.frame(table(rep_table$location1))
sum(rep_counts$Freq) #840

#there has to be a better way to code this but for now.... 
myreps <- c(rep(1:9, 4), rep(1:7, 1), rep(1:9, 10), rep(1:6, 1), rep(1:9, 3), rep(1:8, 1), rep(1:9, 28), rep(1:8, 3), rep(1:9, 20), rep(1:12, 1), rep(1:18, 11))

#####
full_reps <- rep(myreps, 1837)

#now i need a "visit" variable... the data frame must be arranged by AVS first 
rep_table_byASV <- rep_table %>%
  arrange(ASV)

#rep_table_byASV$replicate <- myreps
rep_table_byASV$replicate <- full_reps

rep_table_byASV <- rep_table_byASV[,-c(1:2)] %>%
  tidyr::pivot_wider(names_from = "replicate", values_from = "reads")

rep_table_byASV_1 <- rep_table_byASV[,c(1:2)]
rep_table_byASV_2 <- rep_table_byASV[,c(3:20)]

rep_table_byASV_2[rep_table_byASV_2 > 0] <- 1

occupancy <- rep_table_byASV_2 %>%
  mutate(numb_occupied = rowSums(., na.rm = T),
         n_replicates = rowSums(!is.na(.)),
         prop_occupied = numb_occupied/n_replicates)

occupancy_sites <- cbind(rep_table_byASV_1, occupancy[,21])

occu_df <- occupancy_sites %>%
  group_by(ASV) %>%
  summarise(max_prop = max(prop_occupied))

occu_asv_filter <- occu_df %>%
  filter(max_prop >= 0.3)       #### this values is important! 

asvs_to_keep <- occu_asv_filter$ASV
```

filter the data frame 
```{r}
asv_table_filter4 <- asv_table_filter3 %>%
  filter(ASV %in% asvs_to_keep)
```

what ASVs did this toss out??? 
```{r}
asv_removed2 <- asv_table_filter3 %>%
  filter(!ASV %in% asvs_to_keep)
```

ASVs removed from samples
```{r}
asv_removed2 %>%
  filter(sample_type == "sample") %>%
  ggplot(aes(x=ASV, y=reads, fill=Sample_ID)) +
  geom_bar(stat = "identity") + 
    theme_bw() + 
  theme(
    axis.text.x = element_blank(),
    legend.position = "none",
    legend.title = element_blank()
  )  
```

hmmm... doesn't seem like i should get rid of all these. real occupancy modeling might help here.or maybe run occupancy modeling at just the sample level (not the site).  or maybe save this even until after taxonomic assignment?? 

```{r}
asv_removed2 %>%
  filter(sample_type == "sample") %>% 
  group_by(ASV) %>%
  summarize(total_reads = sum(reads)) %>%
  arrange(desc(total_reads)) %>%
  head()
```

ASV85 = Gadidae
ASV88 = Gadidae
ASV91 = Stichaeus punctatus
ASV95 = Gadus chalcogrammus
etc....  

**this step is when lots of ASVs are dropped from the data set** - not necessarily a bad thing but need to check this


# 5. Dissimilarity between PCR (biological) replicates 

This step removes samples for which the dissimilarity between PCR replicates exceeds the normal distribution of dissimilarities observed in samples. The objective of this step is to remove any technical replicates that look like they do not belong.

following the gruinard_decon again, i'll calculate an eDNA index
```{r}
normalized <- asv_table_filter4 %>%
  group_by(Sample_ID) %>%
  mutate(Tot = sum(reads),
         Prop_reads = reads/Tot) %>%
  dplyr::group_by(ASV) %>%
  mutate(Colmax = max(Prop_reads, na.rm = TRUE),
         Normalized_reads = Prop_reads/Colmax)
```

```{r}
library(vegan)
```

pivot table to have samples by ASV 
```{r}
normalized_pivot <- normalized %>%
  dplyr::select(Sample_ID, sample_type, ASV, Normalized_reads) %>%
  tidyr::pivot_wider(names_from = "ASV", values_from = "Normalized_reads")
```


i need replicate info so split Sample_ID
```{r}
temp <- normalized_pivot %>%
  dplyr::filter(sample_type == "sample") %>%
  tidyr::separate(col = "Sample_ID", into = c("ID", "rep"), sep = "-", remove = F)
```
the warning is for the samples with both extraction and PCR replicates... for now i'll just treat all replicates of those as PCR reps. but should probably deal with this later on. 

```{r}
replicates <- temp$ID
```

```{r}
norm_reads <- temp[,-c(1:4)]

row.names(norm_reads) <- temp$Sample_ID
```

calculate distances between samples 
```{r}
distmat <- vegan::vegdist(norm_reads)
```


this code is copied from 20180220_Tides_and_eDNA_RPK.Rmd
```{r}
distList=list(NA); distList.tri=list(NA); index=1
          for (i in unique(replicates)){
          	rowMatch<-which(replicates%in%i)
          	distList[[index]]<-as.matrix(distmat)[rowMatch, rowMatch]
          			distList.tri[[index]]<-	distList[[index]][upper.tri(distList[[index]])]
          	index=index+1
          }

normparams=MASS::fitdistr(unlist(distList.tri), "normal")$estimate  #fit normal distribution to bray-curtis dissimilarities. lognormal, beta, etc, had less-good fits.
probs=pnorm(unlist(distList.tri), normparams[1], normparams[2])
outliers =which(probs>0.95)
minOutlier<-min(unlist(distList.tri)[outliers]) #minimum outlier value
	
 #remove outliers
      #distList<-distList[-which(lapply(distList, length)==1)] 	   ### this line produces an error - come back and make sure this still works as it should. 
      namesOutliers=list(NA)
      for (i in 1:length(distList)){
      	namesOutliers[[i]]<-intersect(
                                names(which(colSums(distList[[i]]>=minOutlier)>0)),
                                names(which.max(rowMeans(distList[[i]])))
                              )
      }
```


```{r}
replicate_outliers <- unlist(namesOutliers)

decontam <- temp %>%
  dplyr::filter(!Sample_ID %in% replicate_outliers)
```

which samples have low similarity across technical replicates??
```{r}
samples_removed2 <- temp %>%
  dplyr::filter(Sample_ID %in% replicate_outliers)

ids_removed <- samples_removed2$ID
```

33 samples were removed because of dissimilarity.

maybe just plot a few to visualize why... 

these samples have dissimilar pcr replicates... 

```{r}
asv_table_filter4 %>%
  tidyr::separate(col = "Sample_ID", into = c("ID", "rep"), sep = "-", remove = F) %>%
  filter(ID %in% ids_removed) %>%
  group_by(Sample_ID) %>%
  mutate(sum=sum(reads)) %>%
  mutate(prop = reads/sum) %>%
  filter(ID == "e00401") %>%
  ggplot(aes(x=Sample_ID, y=prop, fill=ASV)) +
  geom_bar(stat = "identity") + 
  #facet_wrap(~ID, scales = 'free', ncol = 3) +
  theme_bw() + 
  theme(
    axis.text.x = element_text(angle = 90, hjust = 0.95),
    legend.position = "none",
    legend.title = element_blank()
  )  
```

```{r}
asv_table_filter4 %>%
  tidyr::separate(col = "Sample_ID", into = c("ID", "rep"), sep = "-", remove = F) %>%
  filter(ID %in% ids_removed) %>%
  group_by(Sample_ID) %>%
  mutate(sum=sum(reads)) %>%
  mutate(prop = reads/sum) %>%
  filter(ID == "e00388") %>%
  ggplot(aes(x=Sample_ID, y=prop, fill=ASV)) +
  geom_bar(stat = "identity") + 
  #facet_wrap(~ID, scales = 'free', ncol = 3) +
  theme_bw() + 
  theme(
    axis.text.x = element_text(angle = 90, hjust = 0.95),
    legend.position = "none",
    legend.title = element_blank()
  )  
```

```{r}
asv_table_filter4 %>%
  tidyr::separate(col = "Sample_ID", into = c("ID", "rep"), sep = "-", remove = F) %>%
  filter(ID %in% ids_removed) %>%
  group_by(Sample_ID) %>%
  mutate(sum=sum(reads)) %>%
  mutate(prop = reads/sum) %>%
  filter(ID == "e00525") %>%
  ggplot(aes(x=Sample_ID, y=prop, fill=ASV)) +
  geom_bar(stat = "identity") + 
  #facet_wrap(~ID, scales = 'free', ncol = 3) +
  theme_bw() + 
  theme(
    axis.text.x = element_text(angle = 90, hjust = 0.95),
    legend.position = "none",
    legend.title = element_blank()
  )  
```

```{r}
asv_table_filter4 %>%
  tidyr::separate(col = "Sample_ID", into = c("ID", "rep"), sep = "-", remove = F) %>%
  filter(ID %in% ids_removed) %>%
  group_by(Sample_ID) %>%
  mutate(sum=sum(reads)) %>%
  mutate(prop = reads/sum) %>%
  filter(ID == "e02099") %>%
  ggplot(aes(x=Sample_ID, y=prop, fill=ASV)) +
  geom_bar(stat = "identity") + 
  #facet_wrap(~ID, scales = 'free', ncol = 3) +
  theme_bw() + 
  theme(
    axis.text.x = element_text(angle = 90, hjust = 0.95),
    legend.position = "none",
    legend.title = element_blank()
  )  
```

** maybe i want to treat the extraction replicates as two separate things.. i bet a few of these would pass the similarity test then. ** 

compared to sample that do have similar pcr replicates... 

```{r}
asv_table_filter4 %>%
  tidyr::separate(col = "Sample_ID", into = c("ID", "rep"), sep = "-", remove = F) %>%
  group_by(Sample_ID) %>%
  mutate(sum=sum(reads)) %>%
  mutate(prop = reads/sum) %>%
  filter(ID == "e00373") %>%
  ggplot(aes(x=Sample_ID, y=prop, fill=ASV)) +
  geom_bar(stat = "identity") + 
  #facet_wrap(~ID, scales = 'free', ncol = 3) +
  theme_bw() + 
  theme(
    axis.text.x = element_text(angle = 90, hjust = 0.95),
    legend.position = "none",
    legend.title = element_blank()
  )  
```



```{r}
asv_table_filter4 %>%
  tidyr::separate(col = "Sample_ID", into = c("ID", "rep"), sep = "-", remove = F) %>%
  group_by(Sample_ID) %>%
  mutate(sum=sum(reads)) %>%
  mutate(prop = reads/sum) %>%
  filter(ID == "e00565") %>%
  ggplot(aes(x=Sample_ID, y=prop, fill=ASV)) +
  geom_bar(stat = "identity") + 
  #facet_wrap(~ID, scales = 'free', ncol = 3) +
  theme_bw() + 
  theme(
    axis.text.x = element_text(angle = 90, hjust = 0.95),
    legend.position = "none",
    legend.title = element_blank()
  )  
```


```{r}
asv_table_filter4 %>%
  tidyr::separate(col = "Sample_ID", into = c("ID", "rep"), sep = "-", remove = F) %>%
  group_by(Sample_ID) %>%
  mutate(sum=sum(reads)) %>%
  mutate(prop = reads/sum) %>%
  filter(ID == "e02080") %>%
  ggplot(aes(x=Sample_ID, y=prop, fill=ASV)) +
  geom_bar(stat = "identity") + 
  #facet_wrap(~ID, scales = 'free', ncol = 3) +
  theme_bw() + 
  theme(
    axis.text.x = element_text(angle = 90, hjust = 0.95),
    legend.position = "none",
    legend.title = element_blank()
  )  
```

filter table 
```{r}
asv_table_filter5 <- asv_table_filter4 %>%
    dplyr::filter(!Sample_ID %in% replicate_outliers)
```

pivot 
```{r}
asv_table_filter5_wide <- asv_table_filter5 %>%
  pivot_wider(names_from = "ASV", values_from = "reads")
```
- 913 reps and 167 ASVs 




