---
title: "Decontaminating GOA pcod eDNA ASVs and samples"
author: "Kimberly Ledger"
date: "2023-06-27"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

This script is an updated version of my decontamination pipeline for the GOA pcod 2021 samples. 

Inputs: This code starts with the ASV table output from DADA2 pipeline in my sequence_filtering.Rmd (eDNA_metabarcoding R project)
Outputs: We will end up with a decontaminated ASV table that can be used for additional analyses.

Decontamination will involve **4 steps**
**1. Estimate tag-jumping** - There is the potential for some barcodes (that are used to identify the individual samples in a MiSeq run) to be assigned to the wrong sample. To estimate how many reads did this, we can use our positive control (which has a known composition and is extremely unlikely to be present in the environmental samples). What we will do is subtract the proportion of reads observed in the control samples from each environmental sample. The output will be a dataset with the same number of samples and ASVs as before, but with fewer reads of certain sequences (ASVs). 
**2. Discard PCR replicates with low numbers of reads** - Sometimes PCR replicates have low read numbers, and therefore will have skewed relative read proportions. These should be removed.  To do this we will fit the number of reads assigned to each sample to a normal distribution and discard those samples with a **97.5%** probability of not fitting in that distribution. The output will be a dataset with fewer samples and potentially fewer ASVs.
**3. Account for contaminants in positive and negative controls** - We can use the reads that show up where we know they shouldn't be (i.e. the controls) to further clean up the dataset. Each ASV found in the controls at a higher abundance than in replicates from environmental samples will be removed. The output will be a dataset with the same number of samples as before but with fewer ASVs.
**4. Hierarchical Occupancy Modeling** - We can use occupancy modeling to help determine if rare ASVs are real or a PCR artifact. We will remove the ASVs that probably aren't real. The output will be a dataset with the same number of samples as before but with fewer ASVs.
**5. Dissimilarity between PCR (biological) replicates** -  PCR replicates should be similar (this is not the case for biological replicates because of the many stochastic processes involved in the sequencing of a sample, but anyways...). This step removes samples for which the dissimilarity between PCR replicates exceeds the normal distribution of dissimilarities observed in samples. The objective of this step is to remove any technical replicates that look like they do not belong. The output will be a dataset with fewer samples and potentially fewer ASVs. 


# Load libraries and data 

load libraries 
```{r}
#library(fitdistrplus) #for fitdist()
library(stringi)
library(rstan)
library(broom)
library(tibble)
#library(bayesplot)
library(tidyverse)
library(vegan)
```

load ASV table and metadata
```{r}
asv_table <- read.csv("/genetics/edna/workdir/GOApcod_2021/combined/trimmed/filtered/outputs/ASVtable.csv") %>%
  rename(Sample_ID = X)

asv_table$Sample_ID <- as.factor(asv_table$Sample_ID)

metadata <- read.csv("/genetics/edna/workdir/GOApcod_2021/GOA2021_metadata_20230515.csv")

#illumina output changed "_" to "-"
metadata$Sample_ID <- gsub("_", "-", metadata$Sample_ID) 
```

starting number of ASV's = 1859
starting number of samples = 1014 (enviro samples, positives, and negatives)

break down of the number of environmental samples, positives, negatives
```{r}
rep_summary <- metadata %>%
  group_by(sample_type) %>%
  summarize(starting_reps = n())
rep_summary
```


# 1. Estimate index hopping  
subtract the proportion of reads that jumped into the control samples from each environmental sample 

add column to the ASV table that labels the sample type
```{r}
asv_table_with_sample_type <- metadata %>%
  dplyr::select(Sample_ID, sample_type) %>%
  left_join(asv_table, by = "Sample_ID")
```

let's start by visualizing the reads in the positive control samples 
```{r}
asv_table_with_sample_type %>%
  pivot_longer(cols = c(3:1861), names_to = "ASV", values_to = "reads") %>%
  filter(sample_type == "positive_control") %>%
 # group_by(Sample_ID) %>%
 # mutate(sum=sum(reads)) %>%
 # mutate(prop = reads/sum) %>%
  ggplot(aes(x=Sample_ID, y=reads, fill=ASV)) +
  geom_bar(stat = "identity") + 
    theme_bw() +
  labs(
    y = "number of sequencing reads",
    x = "sample ID",
    title = "ASV reads") + 
  theme(
    axis.text.x = element_text(angle = 90, hjust = 0.95),
    legend.text = element_text(size = 8),
    legend.key.size = unit(0.3, "cm"),
    legend.position = "none",
    legend.title = element_blank()
  )
```

identify the maximum proportion of reads for each ASV found in the positive controls
```{r}
prop_asvs_in_positives <- asv_table_with_sample_type %>%
  filter(sample_type == "positive_control") %>%
  pivot_longer(cols = c(3:1861), names_to = "ASV", values_to = "reads") %>%
  group_by(Sample_ID) %>%
  mutate(TotalReadsPerSample = sum(reads)) %>%
  mutate(Prop = reads/TotalReadsPerSample) %>%
  group_by(ASV) %>%
  summarise(max_prop = max(Prop))
```

now subtract this max proportion for each ASV from environmental samples and all negative controls  
```{r}
indexhop_table <- asv_table_with_sample_type %>%
  #filter(sample_type != "positive_control") %>% ## working all samples except the positive controls 
  pivot_longer(cols = c(3:1861), names_to = "ASV", values_to = "reads") %>%
  group_by(Sample_ID) %>%
  mutate(TotalReadsPerSample = sum(reads)) %>%
  left_join(prop_asvs_in_positives, by = "ASV") %>%
  mutate(IndexHoppingReads = TotalReadsPerSample*max_prop) %>%
  mutate(reads_IndexHop_removed = reads - IndexHoppingReads) %>%
  mutate(reads_IndexHop_removed = if_else(reads_IndexHop_removed < 0, 0, reads_IndexHop_removed))
```

clean up the table by removing columns no longer needed 
```{r}
asv_table_filter1 <- indexhop_table %>%
  dplyr::select(Sample_ID, sample_type, ASV, reads_IndexHop_removed) %>%
  rename(reads = reads_IndexHop_removed)
head(asv_table_filter1)
```

this is a summary of the number of reads removed by ASV and sample_ID
```{r}
decontaminated_1 <- indexhop_table %>%
  dplyr::select(Sample_ID, ASV, IndexHoppingReads) %>%
  pivot_wider(names_from = "ASV", values_from = "IndexHoppingReads")
head(decontaminated_1)
```

and a list of the proportion of reads from ASVs removed 
```{r}
prop_removed_1 <- prop_asvs_in_positives %>%
  arrange(desc(max_prop))
head(prop_removed_1)
```

apart from ASV16 which is for sturgeon (the positive control) and a few ASVs at about 2.5%, the max proportion of reads removed is just over 1% 


# 2. Discard PCR replicates with low numbers of reads 

calculate reads per sample
```{r}
all_reads <- asv_table_filter1 %>%
  group_by(Sample_ID) %>%
  summarize(ReadsPerSample = sum(reads))
```

visualize 
```{r}
all_reads$x_reordered <- reorder(all_reads$Sample_ID, -all_reads$ReadsPerSample)

all_reads %>%
  ggplot(aes(x = x_reordered, y = ReadsPerSample)) + 
  geom_bar(stat = "identity")
```

fit a normal distribution
```{r}
#fit <- fitdist(all_reads$ReadsPerSample, "gamma", lower=c(0,0), start=list(scale=1,shape=1))

fit <- MASS::fitdistr(all_reads$ReadsPerSample, "normal")


#all_reads %>%  
#  mutate(prob = pgamma(ReadsPerSample, shape = fit$estimate[[2]], scale = fit$estimate[[1]], lower.tail = TRUE,
#       log.p = FALSE)) -> all_reads

all_reads %>%  
  mutate(prob = pnorm(all_reads$ReadsPerSample, fit$estimate[[1]], fit$estimate[[2]])) -> all_reads
```

identify and remove the outliers - parameters used in gruinard_decon
```{r}
low_dist_probability_cutoff <- 0.025
minimum_read_cutoff <- 1000

outliers <- all_reads %>% 
  filter(prob < low_dist_probability_cutoff  | ReadsPerSample < minimum_read_cutoff) # changed to 0.05 to save the two samples
outlierIDs <- outliers$Sample_ID
```

which samples are removed because of the 2.5%/1000 reads threshold??
```{r}
replicates_removed_2 <- asv_table_filter1 %>%
  filter(Sample_ID %in% outlierIDs) %>%
  pivot_wider(names_from = "ASV", values_from = "reads")
head(replicates_removed_2)
```

number of pcr replicates removed
```{r}
nrow(replicates_removed_2)
```

plot them
```{r}
replicates_removed_2 %>%
  pivot_longer(cols = c(3:143), names_to = "ASV", values_to = "count") %>%
ggplot(aes(x=Sample_ID, y=count, fill=ASV)) +
  geom_bar(stat = "identity") + 
    theme_bw() + 
  theme(
    axis.text.x = element_blank(),
    legend.position = "none",
    legend.title = element_blank()
  )  
```

double check that the probability threshold is appropriate. i.e. make sure no replicates with lots of reads (usually >2000 are removed)

filter the data frame 
```{r}
asv_table_filter2 <- asv_table_filter1 %>%
  filter(!Sample_ID %in% outlierIDs)
```

how many environmental samples, control samples did we get rid of here? 
```{r}
removed_2_summary <- replicates_removed_2 %>%
  group_by(sample_type) %>%
  summarize(removed_2 = n())
removed_2_summary
```

join to summary table 
```{r}
rep_summary %>%
  left_join(removed_2_summary)
```

did we lose any ASVs during this step? 
```{r}
length(unique(asv_table_filter2$ASV)) 
```
Nope, 1859 is what we started with. 


# 3. Account for contaminants in positive and negative controls 

here we will remove ASVs that occur more frequently in positive and negative controls. 

since decontamination step 2 removed all the positive and many negative controls, i will need to go back to the starting read data to identify these ASVs. 

For each ASV, we calculate the maximum proportion, mean proportion, and total number of reads in all samples. While it's helpful to see the read proportions (and maybe i'll return to filtering by proportions sometime...), we will only remove ASVs that have maximum abundance in control replicate. 

number of reads
```{r}
tempA <- asv_table_with_sample_type %>%
  pivot_longer(cols = c(3:1861), names_to = "ASV", values_to = "reads") %>%
  group_by(ASV, sample_type) %>%
  summarize(TotalReadsPerASV = sum(reads)) %>%
  arrange(ASV)
```
proportion of read
```{r}
tempB <-  asv_table_with_sample_type %>%
  pivot_longer(cols = c(3:1861), names_to = "ASV", values_to = "reads") %>%
  group_by(Sample_ID) %>%
  mutate(TotalReadsPerSample = sum(reads)) %>%
  mutate(Prop = reads/TotalReadsPerSample) %>%
  group_by(sample_type, ASV) %>%
  summarise(max_prop = max(Prop),
            mean_prop = mean(Prop))
```

```{r}
tempC <- tempA %>%
  left_join(tempB, by = c("sample_type", "ASV")) %>%
  pivot_wider(names_from = "sample_type", values_from = c("TotalReadsPerASV", "max_prop", "mean_prop"))
```

what ASVs have greater read numbers in positives than samples? 
```{r}
tempC %>%
  filter(TotalReadsPerASV_positive_control > TotalReadsPerASV_sample)
```

what ASVs have greater read numbers in extraction blanks than samples? 
```{r}
tempC %>%
  filter(TotalReadsPerASV_extraction_blank > TotalReadsPerASV_sample)
```


what ASVs have greater read numbers in PCR blanks than samples? 
```{r}
tempC %>%
  filter(TotalReadsPerASV_PCR_blank > TotalReadsPerASV_sample)
```

what ASVs have greater read numbers in field blanks than samples? 
```{r}
tempC %>%
  filter(TotalReadsPerASV_field_blank > TotalReadsPerASV_sample)
```

remove any ASV that has a *total number of reads* greater in the controls than in real samples.

```{r}
tempD <- tempC %>%
  filter(TotalReadsPerASV_positive_control > TotalReadsPerASV_sample | TotalReadsPerASV_extraction_blank > TotalReadsPerASV_sample | TotalReadsPerASV_PCR_blank > TotalReadsPerASV_sample | TotalReadsPerASV_field_blank > TotalReadsPerASV_sample)

asvs_to_remove <- tempD$ASV  #31 ASVs removed (from original count)
```

filter the data frame 
```{r}
asv_table_filter3 <- asv_table_filter2 %>%
  filter(!ASV %in% asvs_to_remove)
```

number of ASVs remaining
```{r}
length(unique(asv_table_filter3$ASV)) 
```

# 4. Hierarchical Occupancy Modeling 

this is based on work by Ryan Kelly: https://github.com/invertdna/OccupancyModeling_Stan/tree/master

the hierarchical stan model used here: https://github.com/zjgold/gruinard_decon/blob/master/gruinard_decontam_script.R
```{r}
##Stan Model
sink("Stan_SOM_hierarchical_with_occuprob.stan")
cat(
  "data{/////////////////////////////////////////////////////////////////////
    int<lower=1> S;    // number of samples (nrow)
    int<lower=1> Species[S];    // index of species, each of which will have a different value for p11 and p10
    int<lower=1> Nspecies;    // number of species, each of which will have a different value for p11 and p10
    int<lower=1> L[S];   // index of locations or species/site combinations, each of which will have a different value psi
    int<lower=1> Nloc;   // number of locations or species/site combinations, each of which will have a different value psi
    int<lower=1> K[S];   // number of replicates per site (ncol)
    int<lower=0> N[S]; // number of detections among these replicates
    int z[S];   // integer flag to help estimate psi parameter
}

parameters{/////////////////////////////////////////////////////////////////////
    real<lower=0,upper=1> psi[Nloc];  //commonness parameter
    real<lower=0,upper=1> p11[Nspecies]; //true positive detection rate
    real<lower=0,upper=1> p10[Nspecies]; //false positive detection rate
}

transformed parameters{/////////////////////////////////////////////////////////////////////
}

model{/////////////////////////////////////////////////////////////////////
  real p[S];
  
    for (i in 1:S){
			z[i] ~ bernoulli(psi[L[i]]);
			p[i] = z[i]*p11[Species[i]] + (1-z[i])*p10[Species[i]];
			N[i] ~ binomial(K[i], p[i]);
	}; 
  
  //priors
  psi ~ beta(2,2); 
  p11 ~ beta(2,2); 
  p10 ~ beta(1,10);
}

generated quantities{
  real<lower=0,upper=1> Occupancy_prob[S];    //after inferring parameters above, now calculate occupancy probability for each observation. Equation from Lahoz-Monfort et al. 2015
  
  for (i in 1:S){
  Occupancy_prob[i]  = (psi[L[i]]*(p11[Species[i]]^N[i])*(1-p11[Species[i]])^(K[i]-N[i])) 
  / ((psi[L[i]]*(p11[Species[i]]^N[i])*(1-p11[Species[i]])^(K[i]-N[i])) 
  + (((1-psi[L[i]])*(p10[Species[i]]^N[i]))*((1-p10[Species[i]])^(K[i]-N[i])))
  );
  }
 }
  
",
fill=TRUE)
sink()

```


the first step is to format my data for the stan model - i need site information so will start by getting that from the metadata
```{r}
sites <- metadata %>%
  dplyr::select(Sample_ID, extraction_ID, pcr_replicate, location1)

rep_table <- asv_table_filter3 %>%
  left_join(sites, by = "Sample_ID") %>%
  filter(location1 != 'NA') %>%            ##remove sample for which we don't have site locations
  arrange(location1)

occu_df <- rep_table %>%
  ungroup() %>%  
  mutate(reads = ifelse(reads > 0, 1, 0)) %>% # change counts to presence/absence
  dplyr::select(ASV, location1, extraction_ID, pcr_replicate, reads) %>%
  group_by(ASV, location1, extraction_ID) %>%
  summarise(K = n(),  #count the number of rows for K 
            N = sum(reads)) %>% #sum the detections (now in reads column) for N 
  rename(Site = location1,
         BiologicalRep = extraction_ID) %>% ## just renaming so that it matches the naming used in the stan model
  separate(ASV, into = c(NA, "Species"), sep = 3, remove = FALSE)

occu_df$Species <- as.integer(occu_df$Species) #convert ASV to an integer

occu_df <- occu_df %>%
  arrange(Species)
```

running the occupancy model on this entire data set will take a VERY long time, so now I will reduce the data set down to the patterns of presence (many ASVs/species have identical patterns of presence, aka. pattern of technical reps)

```{r}
pattern.of.replication <- rep_table %>%
  ungroup() %>%  
  mutate(reads = ifelse(reads > 0, 1, 0)) %>% # change counts to presence/absence
  filter(!grepl("-2-", Sample_ID)) %>%    ## the extraction replicates are messing thing up right now... i need to code this better in the metadata/etc eventually 
  dplyr::select(location1, extraction_ID, pcr_replicate, ASV, reads) %>%
  pivot_wider(names_from = pcr_replicate, values_from = reads) %>%
  mutate(ndetections = A + B + C) %>%
  group_by(location1, ndetections, ASV) %>%
  summarize(tot = sum(!is.na(ndetections)))

pattern.of.presense <- pattern.of.replication %>%
  spread(ndetections, tot, fill = 0) %>%
  unite(repetition.level, '0', '1', '2', '3', sep = '.') %>%
  select(!`<NA>`)

#select a representative 
unique.pattern <- pattern.of.presense %>%
  group_by(repetition.level) %>%
  summarise(ASV = head(ASV,1) ,
            Site = head(location1, 1)) %>%
  unite(Site, ASV, col = 'key', sep = '.', remove = F)


#subset my full data frame (occu_df) to just include one representative of each unique detection pattern 
occu_df_subset <- occu_df %>%
  unite(Site, ASV, col = 'key', sep = '.', remove = F) %>%
  filter(key %in% unique.pattern$key) 
```


```{r}
temp.df <- occu_df_subset

  #make a new species column to that values are consecutive
  Species <- temp.df$Species
  temp.df$Species_1 <- match(Species, unique(Species))

  #create unique identifier for combinations of site-biologicalrep-ASV; for use in hierarchical modeling
  SDS <- unite(data = temp.df, col = SDS, c("Site", "BiologicalRep", "Species")) %>% pull(SDS)
  temp.df$SiteRepSpecies <- match(SDS, unique(SDS)) #index for unique site-biologicalrep-species combinations
  
  #create unique identifier for combinations of site-ASV; for use in hierarchical modeling
  SS <- unite(data = temp.df, col = SS, c("Site", "Species")) %>% pull(SS)
  temp.df$SiteSpecies <- match(SS, unique(SS)) #index for unique site-species combinations
  
  #####################
  #run Stan model
  #note this will take a while the first time you run a particular model, because it needs to compile from C++
  #####################      
  myHierarchicalModel <- stan(file = "Stan_SOM_hierarchical_with_occuprob.stan", 
                        data = list(
                          S = nrow(temp.df),
                          Species = temp.df$Species_1,
                          Nspecies = length(unique(temp.df$Species_1)),
                          L = temp.df$SiteSpecies,
                          Nloc = length(unique(temp.df$SiteSpecies)),
                          K = temp.df$K,
                          N = temp.df$N,
                          z = ifelse(temp.df$N > 0, 1, 0)
                             ), 
                             chains = 4,   #number of chains
                             iter = 4000   #number of iterations per chain
       )
       
  myHierarchicalStanResults <- tidy(tibble(as.data.frame(myHierarchicalModel)))
  
  write_rds(myHierarchicalStanResults, "occupancy_output_20230629.rds")  ## not that this version actually takes very long to rerun but anyways... 
```


```{r}
#myHierarchicalStanResults <- read_rds("occupancy_output_20230629.rds")

  ## occupancy probabilities 
  myHierarchicalStanResults_occu <- myHierarchicalStanResults %>%
    filter(grepl("Occupancy_prob", column)) %>%
    separate(column, into=c("column","SiteRepSpecies"), sep="([\\[\\]])")
  
  myHierarchicalStanResults_occu$SiteRepSpecies <- as.numeric(myHierarchicalStanResults_occu$SiteRepSpecies)
  
  occupancy_prob <- temp.df %>% 
    select(ASV, Species, Site, SiteSpecies, SiteRepSpecies) %>%
    left_join(myHierarchicalStanResults_occu, by = "SiteRepSpecies") %>% 
    group_by(ASV, Site, SiteSpecies) %>%
    summarise(max_Occupancy_prob = max(mean))
  
# join my occupancy probabilities the unique.pattern df and then the pattern of presence... 
occu_with_key <- occupancy_prob %>%
  unite(Site, ASV, col = 'key', sep = '.', remove = F) %>%
  left_join(unique.pattern, by = c('key', 'ASV', 'Site')) 

occu_with_key_to_join <- occu_with_key[, c("repetition.level", "max_Occupancy_prob")]

site.asv_occupancy_probs <- pattern.of.presense %>%
  left_join(occu_with_key_to_join)

keepers <- site.asv_occupancy_probs %>%
  filter(max_Occupancy_prob >= 0.8) %>%
  unite(location1, ASV, col = 'filter_id', sep = '.', remove = F)
discard <- site.asv_occupancy_probs %>%
  filter(max_Occupancy_prob < 0.8)
```

this will cut down on my ASVs across sites A LOT! - which i think is a good thing... 

```{r}
asv_table_filter4 <- rep_table %>% 
  unite(location1, ASV, col = 'loc.asv', sep = '.', remove = F) %>%
  filter(loc.asv %in% keepers$filter_id)
```

note: samples that are extraction replicates are now back in the data set. i'll need to make sure they are filtered correctly at some point... 

how many unique ASVs are now in the data set? 
```{r}
length(unique(asv_table_filter4$ASV)) 
```

1828-209 = 1619 ASVs removed. that's ~88%... 

### maybe add a step here that joins the dataset to taxonomy and remove any additional ASVs (such as from mammals, if they remain?)


# 5. Dissimilarity between PCR (biological) replicates 

This step removes samples for which the dissimilarity between PCR replicates exceeds the normal distribution of dissimilarities observed in samples. The objective of this step is to remove any technical replicates that look like they do not belong.


before really starting this step, let me deal with the pcr/extraction replicate messiness with a few of the samples... 
```{r}
asv_table_filter4 <- asv_table_filter4 %>%
  tidyr::separate(col = "Sample_ID", into = c("extraction_ID", "rep_id", "rep2"), sep = "-", remove = F)

no_ext_reps <- asv_table_filter4[1:13508,]
ext_reps <- asv_table_filter4[13509:17768,]

ext_reps <- ext_reps %>%
  unite(rep_id, rep2, sep = "_", col = "rep_id") %>%
  mutate(rep_id = ifelse(rep_id == "1_A", "A", 
                       ifelse(rep_id == "1_B", "B",
                              ifelse(rep_id == "1_C", "C",
                                     ifelse(rep_id == "2_A", "D",
                                            ifelse(rep_id == "2_B", "E",
                                                   ifelse(rep_id == "2_C", "F", rep_id)))))))
  
no_ext_reps <- no_ext_reps[,-4]

combined <- bind_rows(no_ext_reps, ext_reps)
```

how many pcr replicates does each biological replicate (aka extraction replicate) have? 
```{r}
combined %>%
  group_by(extraction_ID) %>%
  summarise(nrep = n_distinct(Sample_ID)) %>%
  #filter(nrep == 2)  # there are 6 
  filter(nrep == 1) # there are zero! 
```

okay, so there are 6 extraction_ID with only 2 pcr replicates. and no extraction_ID with only 1 pcr replicate. 


first, i'll calculate an eDNA index
```{r}
normalized <- combined %>%
  group_by(Sample_ID) %>%
  mutate(Tot = sum(reads),
         Prop_reads = reads/Tot) %>%
  dplyr::group_by(ASV) %>%
  mutate(Colmax = max(Prop_reads, na.rm = TRUE),
         Normalized_reads = Prop_reads/Colmax)

#add a new sample id column that also includes the location - will use this for dissimilarity measures
normalized <- normalized %>%
  unite(site_extraction, location1, extraction_ID, sep = "_", remove = FALSE) %>%
  unite(new_ID, site_extraction, rep_id, sep = "-", remove = FALSE)
```


```{r}
tibble_to_matrix <- function (tb) {
  
  tb %>%
  #normalized %>%
    group_by(new_ID, ASV) %>% 
    summarise(nReads = sum(Normalized_reads)) %>% 
    spread ( key = "ASV", value = "nReads", fill = 0) %>%
    ungroup() -> matrix_1
    samples <- pull (matrix_1, new_ID)
    matrix_1[,-1] -> matrix_1
    data.matrix(matrix_1) -> matrix_1
    dimnames(matrix_1)[[1]] <- samples
    vegdist(matrix_1) -> matrix_1
}

```


```{r}
all.distances.full <- tibble_to_matrix(normalized)

# Do all samples have a name?
summary(is.na(names(all.distances.full)))
```

make the pairwise distances a long table
```{r}
library(reshape)
as_tibble(subset(melt(as.matrix(all.distances.full)))) -> all.distances.melted

# Any major screw ups
summary(is.na(all.distances.melted$value))

# Now, create a three variables for all distances, they could be PCR replicates, BIOL replicates, or from the same site

all.distances.melted %>%
  separate (X1, into = "Bottle1", sep = "\\-", remove = FALSE) %>%
  separate (Bottle1, into = "Site1", remove = FALSE) %>%
  separate (X2, into ="Bottle2", sep = "\\-", remove = FALSE) %>%
  separate (Bottle2, into = "Site2", remove = FALSE) %>%
  mutate ( #Day.site1 = str_sub(Bottle1, start = 1, end = -2),
           #Day.site2 = str_sub(Bottle2, start = 1, end = -2),
           Distance.type = case_when( Bottle1 == Bottle2 ~ "PCR.replicates",
                                      #Day.site1 == Day.site2 ~ "Biol.replicates",
                                      Site1 == Site2 ~ "Same Site",
                                      TRUE ~ "Different Site"
                                     )) %>%
  dplyr::select(Sample1 = X1, Sample2 = X2 , value , Distance.type) %>%
  filter (Sample1 != Sample2) -> all.distances.to.plot

# Checking all went well

sapply(all.distances.to.plot, function(x) summary(is.na(x)))

```

```{r}
all.distances.to.plot$Distance.type <- all.distances.to.plot$Distance.type  %>% fct_relevel("PCR.replicates", "Same Site")

ggplot (all.distances.to.plot) +
  geom_histogram (aes (fill = Distance.type, x = value, after_stat(ndensity)), position = "dodge",  alpha = 0.9, bins = 50) +
  facet_wrap( ~ Distance.type) +
  labs (x = "Pairwise dissimilarity", y = "density" ,
        Distance.type = "Distance") +
    guides (fill = "none")

  #ggsave("visual.anova.png", dpi = "retina")
```

i think this looks as expected, lowest dissimilarity in pcr reps, then samples from the same site, then samples from different sites. cool. 


will try to follow what was done here (https://github.com/ramongallego/eDNA.and.Ocean.Acidification.Gallego.et.al.2020/blob/master/Scripts/Denoising.all.runs.Rmd) and instead of choosing outliers based on the pairwise distances, we can do a similar thing using the distance to centroid. 


####################################

now identify and discard outliers 
```{r}
normalized %>%
  group_by(site_extraction) %>% nest() -> nested.cleaning      ##group by site-extractionID??? 

nested.cleaning %>% 
  mutate(matrix = map(data, tibble_to_matrix)) -> nested.cleaning

nested.cleaning %>% mutate(ncomparisons = map(matrix, length)) -> nested.cleaning
```

```{r}
dist_to_centroid <- function (x,y) {
  
  #biol <- rep(y, dim(x)[[1]])
  biol <- rep(y, length(x))
  
  if (length(biol) == 1) {
    output = rep(x[1]/2,2)
    names(output) <- attr(x, "Labels")
  }else{ 
    
  dispersion <- betadisper(x, group = biol)
  output = dispersion$distances
  }
  output
    }
```

```{r}
nested.cleaning.temp <- nested.cleaning %>%
  filter(ncomparisons < 4)

nested.cleaning.temp2 <- nested.cleaning.temp %>% mutate (distances = map2(matrix, site_extraction, dist_to_centroid)) ### ERROR HERE ####  this doesn't like the site-extractions with six replicates.... 

all_distances <- nested.cleaning.temp2 %>%
  unnest_longer(distances) %>%
  dplyr::select(site_extraction, distances_id, distances)

hist(all_distances$distances)
```

calculate normal distribution of distances to centroid
```{r}
normparams <- MASS::fitdistr(all_distances$distances, "normal")$estimate                                      
probs <- pnorm(all_distances$distances, normparams[1], normparams[2])
outliers_centroid <- which(probs>0.95)

discard_centroid <- all_distances$distances_id[outliers_centroid]
discard_centroid
```

####################### 


here i will just filter using the pairwise distances among PCR replicates
```{r}
pcr.distances <- all.distances.to.plot %>%
  filter(Distance.type == "PCR.replicates")

pcr.distances.value <- pcr.distances$value

#normparams <- fitdistr(all_pairwise_distances, "normal")$estimate
normparams <- MASS::fitdistr(pcr.distances.value, "normal")$estimate                                      
#  probs <- pnorm(all_pairwise_distances, normparams[1], normparams[2])
probs <- pnorm(pcr.distances.value, normparams[1], normparams[2])
outliers <- which(probs>0.95)

discard <- pcr.distances[outliers,]

samples_to_discard <- data.frame(unique(c(discard$Sample1, discard$Sample2)))
colnames(samples_to_discard) <- "new_ID"
```

output the samples that pass this filter
```{r}
asv_table_filter5 <- combined %>%
  unite(site_extraction, location1, extraction_ID, sep = "_", remove = FALSE) %>%
  unite(new_ID, site_extraction, rep_id, sep = "-", remove = FALSE) %>%
  filter(!new_ID %in% discard_centroid)
```


which extraction IDs had a pcr replicate with low similarity??
```{r}
to_discard <- data.frame(discard_centroid) %>%
  separate(discard_centroid, into = c("location1", "extraction_ID", "rep_ID"))

removed_step5 <- combined %>%
  filter(extraction_ID %in% to_discard$extraction_ID)
```


after my first round of filtering, i wonder if the 95% threshold is too stringent... let's plot what pairwise distances were removed
```{r}
pcr.distances$probs <- probs 
pcr.distances

#what is the minimum pairwise distance that is removed by the 95% threshold? 
min_value <- min(pcr.distances$value[pcr.distances$probs > 0.95])

ggplot (pcr.distances) +
  geom_histogram (aes ( x = value, after_stat(ndensity)), position = "dodge",  alpha = 0.9, bins = 50) +
  geom_vline(xintercept = min_value, linetype = "dashed", color = "red") +
  labs (x = "Pairwise dissimilarity", y = "density" ,
        Distance.type = "Distance") +
    guides (fill = "none")

```

okay to convince me if this threshold needs to be moved, let's visualize the samples that are just at this limit...
```{r}
x <- pcr.distances %>%
  filter(value >0.75, value < 0.80) %>%
  arrange(value)
x
```

```{r}
asv_table_filter4 %>%
  group_by(Sample_ID) %>%
  mutate(sum=sum(reads)) %>%
  mutate(prop = reads/sum) %>%
  filter(location1 == 100) %>%
  ggplot(aes(x=Sample_ID, y=prop, fill=ASV)) +
  geom_bar(stat = "identity") + 
  facet_wrap(~extraction_ID, scales = 'free', ncol = 3) +
  theme_bw() + 
  theme(
    axis.text.x = element_text(angle = 90, hjust = 0.95),
    legend.position = "none",
    legend.title = element_blank()
  )  
```

e02091-1-A and e02091-1-B don't pass the 95% filter... 

```{r}
asv_table_filter4 %>%
  group_by(Sample_ID) %>%
  mutate(sum=sum(reads)) %>%
  mutate(prop = reads/sum) %>%
  filter(location1 == 103) %>%
  ggplot(aes(x=Sample_ID, y=prop, fill=ASV)) +
  geom_bar(stat = "identity") + 
  facet_wrap(~extraction_ID, scales = 'free', ncol = 3) +
  theme_bw() + 
  theme(
    axis.text.x = element_text(angle = 90, hjust = 0.95),
    legend.position = "none",
    legend.title = element_blank()
  )  
```

e02106-B,C,F; e02107-A,F; e02108-A,D
hmmm.. these don't seem too bad... 

```{r}
asv_table_filter4 %>%
  group_by(Sample_ID) %>%
  mutate(sum=sum(reads)) %>%
  mutate(prop = reads/sum) %>%
  filter(location1 == 37) %>%
  ggplot(aes(x=Sample_ID, y=prop, fill=ASV)) +
  geom_bar(stat = "identity") + 
  facet_wrap(~extraction_ID, scales = 'free', ncol = 3) +
  theme_bw() + 
  theme(
    axis.text.x = element_text(angle = 90, hjust = 0.95),
    legend.position = "none",
    legend.title = element_blank()
  )  
```

e00479-B,C - again doesn't seem too bad.  

how about samples from site 82 - it's pairwise is just below 0.8 
```{r}
asv_table_filter4 %>%
  group_by(Sample_ID) %>%
  mutate(sum=sum(reads)) %>%
  mutate(prop = reads/sum) %>%
  filter(location1 == 82) %>%
  ggplot(aes(x=Sample_ID, y=prop, fill=ASV)) +
  geom_bar(stat = "identity") + 
  facet_wrap(~extraction_ID, scales = 'free', ncol = 3) +
  theme_bw() + 
  theme(
    axis.text.x = element_text(angle = 90, hjust = 0.95),
    legend.position = "none",
    legend.title = element_blank()
  )  
```

e02046-A,C  - those look a bit different... 

MAYBE I DON"T WANT TO FILTER BY PAIRWISE DISTANCES... this seems to drop reps like e02046-A that are similar to other reps(e02046-B) just because they are different from another (e02026-C). when just e02026-C should be the one removed..  


maybe just plot a few. 

these samples have dissimilar pcr replicates... 
```{r}
unique(removed_step5$extraction_ID)

length(unique(removed_step5$extraction_ID))  ## these are just the extraction_ID with at least one PCR rep removed 

first_six <- unique(removed_step5$extraction_ID)[1:6]

removed_step5 %>%
  filter(extraction_ID %in% first_six) %>%
  group_by(Sample_ID) %>%
  mutate(sum=sum(reads)) %>%
  mutate(prop = reads/sum) %>%
  ggplot(aes(x=Sample_ID, y=prop, fill=ASV)) +
  geom_bar(stat = "identity") + 
  facet_wrap(~extraction_ID, scales = 'free', ncol = 3) +
  theme_bw() + 
  theme(
    axis.text.x = element_text(angle = 90, hjust = 0.95),
    legend.position = "none",
    legend.title = element_blank()
  )  
```

okay so the samples from above that are removed are: e00385-B, e00388-B, e00390-B and C, e00396-C, e00409-B, and e00426-A.  This all makes sense visually for these few extraction replicates. cool. 


how similar are these to the other extractions from their locations??? 

```{r}
asv_table_filter4 %>%
  filter(extraction_ID == 'e00388')

asv_table_filter4 %>%
  #filter(!extraction_ID %in% ids_removed) %>%
  group_by(Sample_ID) %>%
  mutate(sum=sum(reads)) %>%
  mutate(prop = reads/sum) %>%
  filter(location1 == 7) %>%
  ggplot(aes(x=Sample_ID, y=prop, fill=ASV)) +
  geom_bar(stat = "identity") + 
  facet_wrap(~extraction_ID, scales = 'free', ncol = 3) +
  theme_bw() + 
  theme(
    axis.text.x = element_text(angle = 90, hjust = 0.95),
    legend.position = "none",
    legend.title = element_blank()
  )  
```

alright none of these look all that similar.. site7 is bit of a mess... 


```{r}
asv_table_filter4 %>%
  filter(extraction_ID == 'e00426')

asv_table_filter4 %>%
  #filter(!extraction_ID %in% ids_removed) %>%
  group_by(Sample_ID) %>%
  mutate(sum=sum(reads)) %>%
  mutate(prop = reads/sum) %>%
  filter(location1 == 27) %>%
  ggplot(aes(x=Sample_ID, y=prop, fill=ASV)) +
  geom_bar(stat = "identity") + 
  facet_wrap(~extraction_ID, scales = 'free', ncol = 3) +
  theme_bw() + 
  theme(
    axis.text.x = element_text(angle = 90, hjust = 0.95),
    legend.position = "none",
    legend.title = element_blank()
  )  
```

pcr reps in e00424 and e00425 are more similar to one another than e00426.  i think we will just drop all pcr reps from e00426 (?).


```{r}
asv_table_filter4 %>%
  filter(extraction_ID == 'e00421')

asv_table_filter4 %>%
  #filter(!extraction_ID %in% ids_removed) %>%
  group_by(Sample_ID) %>%
  mutate(sum=sum(reads)) %>%
  mutate(prop = reads/sum) %>%
  filter(location1 == 29) %>%
  ggplot(aes(x=Sample_ID, y=prop, fill=ASV)) +
  geom_bar(stat = "identity") + 
  facet_wrap(~extraction_ID, scales = 'free', ncol = 3) +
  theme_bw() + 
  theme(
    axis.text.x = element_text(angle = 90, hjust = 0.95),
    legend.position = "none",
    legend.title = element_blank()
  )  
```

the pairwise distance between e00421-A and e00421-C exceeded the threshold...


```{r}
asv_table_filter4 %>%
  filter(extraction_ID == 'e00474')

asv_table_filter4 %>%
  #filter(!extraction_ID %in% ids_removed) %>%
  group_by(Sample_ID) %>%
  mutate(sum=sum(reads)) %>%
  mutate(prop = reads/sum) %>%
  filter(location1 == 36) %>%
  ggplot(aes(x=Sample_ID, y=prop, fill=ASV)) +
  geom_bar(stat = "identity") + 
  facet_wrap(~extraction_ID, scales = 'free', ncol = 3) +
  theme_bw() + 
  theme(
    axis.text.x = element_text(angle = 90, hjust = 0.95),
    legend.position = "none",
    legend.title = element_blank()
  )  
```

the pairwise distance between e00474-A and e00474-C exceeded the threshold...

okay, i could go on like this forever... i need to make a decision now on how exactly to filter the pcr reps. 




PLOTTING THE NEWLY FILTERED DATA! - remember that i ignored the samples with 2 extraction reps for now. so none of those sites have been filtered for pcr rep dissimilarity. 

compared to samples that do have similar pcr replicates... 

**site 1**
```{r}
asv_table_filter5 %>%
  group_by(Sample_ID) %>%
  mutate(sum=sum(reads)) %>%
  mutate(prop = reads/sum) %>%
  filter(location1 == 1) %>%
  ggplot(aes(x=Sample_ID, y=prop, fill=ASV)) +
  geom_bar(stat = "identity") + 
  facet_wrap(~extraction_ID, scales = 'free', ncol = 3) +
  theme_bw() + 
  theme(
    axis.text.x = element_text(angle = 90, hjust = 0.95),
    legend.position = "none",
    legend.title = element_blank()
  )  
```

**site 2**
```{r}
asv_table_filter5 %>%
  group_by(Sample_ID) %>%
  mutate(sum=sum(reads)) %>%
  mutate(prop = reads/sum) %>%
  filter(location1 == 2) %>%
  ggplot(aes(x=Sample_ID, y=prop, fill=ASV)) +
  geom_bar(stat = "identity") + 
  facet_wrap(~extraction_ID, scales = 'free', ncol = 3) +
  theme_bw() + 
  theme(
    axis.text.x = element_text(angle = 90, hjust = 0.95),
    legend.position = "none",
    legend.title = element_blank()
  )  
```

**site 3**
```{r}
asv_table_filter5 %>%
  group_by(Sample_ID) %>%
  mutate(sum=sum(reads)) %>%
  mutate(prop = reads/sum) %>%
  filter(location1 == 3) %>%
  ggplot(aes(x=Sample_ID, y=prop, fill=ASV)) +
  geom_bar(stat = "identity") + 
  facet_wrap(~extraction_ID, scales = 'free', ncol = 3) +
  theme_bw() + 
  theme(
    axis.text.x = element_text(angle = 90, hjust = 0.95),
    legend.position = "none",
    legend.title = element_blank()
  )  
```

**site 7**
```{r}
asv_table_filter5 %>%
  group_by(Sample_ID) %>%
  mutate(sum=sum(reads)) %>%
  mutate(prop = reads/sum) %>%
  filter(location1 == 7) %>%
  ggplot(aes(x=Sample_ID, y=prop, fill=ASV)) +
  geom_bar(stat = "identity") + 
  facet_wrap(~extraction_ID, scales = 'free', ncol = 3) +
  theme_bw() + 
  theme(
    axis.text.x = element_text(angle = 90, hjust = 0.95),
    legend.position = "none",
    legend.title = element_blank()
  )  
```

site 7 is an example where dissimilar pcr reps have been filtered out...

**site 29**
```{r}
asv_table_filter5 %>%
  group_by(Sample_ID) %>%
  mutate(sum=sum(reads)) %>%
  mutate(prop = reads/sum) %>%
  filter(location1 == 29) %>%
  ggplot(aes(x=Sample_ID, y=prop, fill=ASV)) +
  geom_bar(stat = "identity") + 
  facet_wrap(~extraction_ID, scales = 'free', ncol = 3) +
  theme_bw() + 
  theme(
    axis.text.x = element_text(angle = 90, hjust = 0.95),
    legend.position = "none",
    legend.title = element_blank()
  )  
```

**site 60**
```{r}
asv_table_filter5 %>%
  group_by(Sample_ID) %>%
  mutate(sum=sum(reads)) %>%
  mutate(prop = reads/sum) %>%
  filter(location1 == 60) %>%
  ggplot(aes(x=Sample_ID, y=prop, fill=ASV)) +
  geom_bar(stat = "identity") + 
  facet_wrap(~extraction_ID, scales = 'free', ncol = 3) +
  theme_bw() + 
  theme(
    axis.text.x = element_text(angle = 90, hjust = 0.95),
    legend.position = "none",
    legend.title = element_blank()
  )  
```



go back and make sure no bio rep is based on only one pcr rep.

pivot 
```{r}
asv_table_filter5_wide <- asv_table_filter5 %>%
  select(!loc.asv) %>%
  pivot_wider(names_from = "ASV", values_from = "reads")
```


make a table that has the number of site, number of bio reps per site, and number of pcr reps per bio rep. - for starting data and then for this final filtered data set... 




